{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"final_dataset_limited_600_per_category.xlsx\")"
      ],
      "metadata": {
        "id": "swAxQZJjNaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['title', 'overview', 'text'], how='all')"
      ],
      "metadata": {
        "id": "LUkzthu8QM26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['title', 'overview', 'text']] = df[['title', 'overview', 'text']].fillna('')\n",
        "\n",
        "df['fulltext'] = df['title'] + ' ' + df['overview'] + ' ' + df['text']"
      ],
      "metadata": {
        "id": "jxm9L09aQ4xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MTN1zEUYO30K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3XH0pSXGPLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analitics"
      ],
      "metadata": {
        "id": "CrUyUK6HnpAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Инициализируем токенайзер BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Проверка баланса классов\n",
        "category_counts = df['category'].value_counts(normalize=True)\n",
        "\n",
        "# Проверка дубликатов\n",
        "duplicate_count = df.duplicated(subset=['fulltext']).sum()\n",
        "\n",
        "# Category distribution visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "category_counts.plot(kind='bar')\n",
        "plt.title('Category Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Category')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Печатаем основные выводы\n",
        "print(\"=== Анализ текста для BERT ===\")\n",
        "print(f\"Общее количество текстов: {len(df)}\")\n",
        "print(f\"Количество уникальных категорий: {df['category'].nunique()}\")\n",
        "print(f\"Количество дубликатов по полному тексту: {duplicate_count}\")\n",
        "print(\"\\n=== Топ-10 категорий по количеству:\")\n",
        "print(df['category'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "AsEBdub9V938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "3B7TRWNenhwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pFLsCQOiVeEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Загрузка модели и токенизатора\n",
        "model_name = 'DeepPavlov/rubert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Создание колонки fulltext\n",
        "df['fulltext'] = (\n",
        "    df['title'].fillna('') + ' ' +\n",
        "    df['overview'].fillna('') + ' ' +\n",
        "    df['text'].fillna('')\n",
        ")\n",
        "\n",
        "# Разделение на train и test с сохранением пропорций классов\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['category']\n",
        ")\n",
        "\n",
        "# Создание словарей меток\n",
        "label2id = {label: idx for idx, label in enumerate(df['category'].unique())}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "# Преобразование меток\n",
        "train_df['label'] = train_df['category'].map(label2id)\n",
        "test_df['label'] = test_df['category'].map(label2id)\n",
        "\n",
        "# Преобразование в HuggingFace Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df[['fulltext', 'label']])\n",
        "eval_dataset = Dataset.from_pandas(test_df[['fulltext', 'label']])\n",
        "\n",
        "# Токенизация\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['fulltext'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "V4kuxG2kNXU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ruBERT model"
      ],
      "metadata": {
        "id": "PO_hiuzQnSrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "zGM46yhpb_Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "0r3nrdfpccrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "print(TrainingArguments.__module__)\n"
      ],
      "metadata": {
        "id": "Lcojhpk3cfXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Загрузка модели\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
        "\n",
        "# Аргументы обучения (одна эпоха)\n",
        "base_training_args = dict(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    report_to='none',\n",
        "    save_strategy='no',\n",
        "    seed=42,\n",
        "    max_grad_norm=1.0,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100\n",
        ")\n",
        "\n",
        "# Истории метрик\n",
        "acc_history = []\n",
        "f1_history = []\n",
        "best_acc = 0.0\n",
        "best_f1 = 0.0\n",
        "\n",
        "# Early stopping\n",
        "patience = 2\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Цикл по эпохам\n",
        "for epoch in range(10):\n",
        "    print(f\"\\n📘 Эпоха {epoch + 1}/10\")\n",
        "\n",
        "    training_args = TrainingArguments(**base_training_args)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset.shuffle(seed=epoch),\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Оценка\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    acc_history.append(acc)\n",
        "    f1_history.append(f1)\n",
        "\n",
        "    print(f\"✅ Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")\n",
        "    print(classification_report(labels, preds, target_names=[id2label[i] for i in range(len(id2label))]))\n",
        "\n",
        "    # Early stopping по F1\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_acc = acc\n",
        "        no_improve_epochs = 0\n",
        "        model.save_pretrained('./best_model_final')\n",
        "        tokenizer.save_pretrained('./best_model_final')\n",
        "        print(\"💾 Лучшая модель сохранена в ./best_model_final\")\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        print(f\"⚠️  Нет улучшения F1 ({no_improve_epochs}/{patience})\")\n",
        "\n",
        "    if no_improve_epochs >= patience:\n",
        "        print(\"⛔ Early stopping: F1 не улучшается.\")\n",
        "        break\n",
        "\n",
        "# Сохраняем финальную модель\n",
        "model.save_pretrained('./final_model_early_stopped')\n",
        "tokenizer.save_pretrained('./final_model_early_stopped')\n",
        "print(\"🎯 Финальная модель сохранена в ./final_model_early_stopped\")\n",
        "\n",
        "# График\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(acc_history) + 1), acc_history, label='Accuracy')\n",
        "plt.plot(range(1, len(f1_history) + 1), f1_history, label='Macro F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Accuracy & Macro F1 per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hjbf4emBRkzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./best_model_final /content/drive/MyDrive/\n",
        "!cp -r ./final_model_early_stopped /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "0jOhA-UbU0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0T-q_2Dclpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Поиск ключевых слов для каждой категории"
      ],
      "metadata": {
        "id": "abgP17G0bRlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EHb2g5EObatF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from natasha import MorphVocab, Doc, NewsEmbedding, NewsMorphTagger, Segmenter\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# ======= Natasha =======\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "# ======= Стоп-слова =======\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "custom_stopwords = set([\n",
        "    'также', 'который', 'которые', 'например', 'года',\n",
        "    'будет', 'данный', 'далее', 'нужно', 'может','новый', 'мочь',\n",
        "    'россия', 'российский', 'американский', 'украинский'\n",
        "])\n",
        "stop_words = set(russian_stopwords).union(custom_stopwords)\n",
        "\n",
        "# ======= Функция очистки и лемматизации =======\n",
        "def preprocess(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[^А-Яа-яЁё ]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if not word[0].isupper()]\n",
        "    text = ' '.join(tokens)\n",
        "    text = text.lower()\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    lemmas = []\n",
        "    for token in doc.tokens:\n",
        "        if token.pos not in ['CONJ', 'ADP', 'PRCL', 'INTJ'] and \\\n",
        "           len(token.text) > 3 and token.pos != 'PROPN':\n",
        "            token.lemmatize(morph_vocab)\n",
        "            lemma = token.lemma\n",
        "            if lemma not in stop_words and len(lemma) > 3:\n",
        "                lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "# ======= Применяем обработку =======\n",
        "df['lemmas'] = df['text'].progress_apply(preprocess)\n",
        "\n",
        "# ======= Считаем слова по категориям =======\n",
        "theme_word_counts = {}\n",
        "\n",
        "for theme, group in df.groupby('category'):\n",
        "    words = []\n",
        "    for lemmas in group['lemmas']:\n",
        "        words.extend(lemmas)\n",
        "    word_freq = Counter(words)\n",
        "    theme_word_counts[theme] = word_freq.most_common(20)\n",
        "\n",
        "# ======= Вывод результатов =======\n",
        "for theme, words in theme_word_counts.items():\n",
        "    print(f\"\\nКатегория: {theme}\")\n",
        "    for word, count in words:\n",
        "        print(f\"  {word}: {count}\")\n"
      ],
      "metadata": {
        "id": "vv0XF9T1bG4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}