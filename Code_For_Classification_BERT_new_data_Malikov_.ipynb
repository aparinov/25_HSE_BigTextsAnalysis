{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"final_dataset_limited_600_per_category.xlsx\")"
      ],
      "metadata": {
        "id": "swAxQZJjNaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['title', 'overview', 'text'], how='all')"
      ],
      "metadata": {
        "id": "LUkzthu8QM26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['title', 'overview', 'text']] = df[['title', 'overview', 'text']].fillna('')\n",
        "\n",
        "df['fulltext'] = df['title'] + ' ' + df['overview'] + ' ' + df['text']"
      ],
      "metadata": {
        "id": "jxm9L09aQ4xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MTN1zEUYO30K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3XH0pSXGPLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analitics"
      ],
      "metadata": {
        "id": "CrUyUK6HnpAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "category_counts = df['category'].value_counts(normalize=True)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
        "duplicate_count = df.duplicated(subset=['fulltext']).sum()\n",
        "\n",
        "# Category distribution visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "category_counts.plot(kind='bar')\n",
        "plt.title('Category Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Category')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –ü–µ—á–∞—Ç–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã\n",
        "print(\"=== –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è BERT ===\")\n",
        "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(df)}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {df['category'].nunique()}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É: {duplicate_count}\")\n",
        "print(\"\\n=== –¢–æ–ø-10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É:\")\n",
        "print(df['category'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "AsEBdub9V938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "3B7TRWNenhwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pFLsCQOiVeEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "model_name = 'DeepPavlov/rubert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ fulltext\n",
        "df['fulltext'] = (\n",
        "    df['title'].fillna('') + ' ' +\n",
        "    df['overview'].fillna('') + ' ' +\n",
        "    df['text'].fillna('')\n",
        ")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ test —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['category']\n",
        ")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π –º–µ—Ç–æ–∫\n",
        "label2id = {label: idx for idx, label in enumerate(df['category'].unique())}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫\n",
        "train_df['label'] = train_df['category'].map(label2id)\n",
        "test_df['label'] = test_df['category'].map(label2id)\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ HuggingFace Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df[['fulltext', 'label']])\n",
        "eval_dataset = Dataset.from_pandas(test_df[['fulltext', 'label']])\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['fulltext'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "V4kuxG2kNXU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ruBERT model"
      ],
      "metadata": {
        "id": "PO_hiuzQnSrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "zGM46yhpb_Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "0r3nrdfpccrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "print(TrainingArguments.__module__)\n"
      ],
      "metadata": {
        "id": "Lcojhpk3cfXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
        "\n",
        "# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–æ–¥–Ω–∞ —ç–ø–æ—Ö–∞)\n",
        "base_training_args = dict(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    report_to='none',\n",
        "    save_strategy='no',\n",
        "    seed=42,\n",
        "    max_grad_norm=1.0,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100\n",
        ")\n",
        "\n",
        "# –ò—Å—Ç–æ—Ä–∏–∏ –º–µ—Ç—Ä–∏–∫\n",
        "acc_history = []\n",
        "f1_history = []\n",
        "best_acc = 0.0\n",
        "best_f1 = 0.0\n",
        "\n",
        "# Early stopping\n",
        "patience = 2\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# –¶–∏–∫–ª –ø–æ —ç–ø–æ—Ö–∞–º\n",
        "for epoch in range(10):\n",
        "    print(f\"\\nüìò –≠–ø–æ—Ö–∞ {epoch + 1}/10\")\n",
        "\n",
        "    training_args = TrainingArguments(**base_training_args)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset.shuffle(seed=epoch),\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # –û—Ü–µ–Ω–∫–∞\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    acc_history.append(acc)\n",
        "    f1_history.append(f1)\n",
        "\n",
        "    print(f\"‚úÖ Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")\n",
        "    print(classification_report(labels, preds, target_names=[id2label[i] for i in range(len(id2label))]))\n",
        "\n",
        "    # Early stopping –ø–æ F1\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_acc = acc\n",
        "        no_improve_epochs = 0\n",
        "        model.save_pretrained('./best_model_final')\n",
        "        tokenizer.save_pretrained('./best_model_final')\n",
        "        print(\"üíæ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./best_model_final\")\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        print(f\"‚ö†Ô∏è  –ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è F1 ({no_improve_epochs}/{patience})\")\n",
        "\n",
        "    if no_improve_epochs >= patience:\n",
        "        print(\"‚õî Early stopping: F1 –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è.\")\n",
        "        break\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "model.save_pretrained('./final_model_early_stopped')\n",
        "tokenizer.save_pretrained('./final_model_early_stopped')\n",
        "print(\"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./final_model_early_stopped\")\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(acc_history) + 1), acc_history, label='Accuracy')\n",
        "plt.plot(range(1, len(f1_history) + 1), f1_history, label='Macro F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Accuracy & Macro F1 per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hjbf4emBRkzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./best_model_final /content/drive/MyDrive/\n",
        "!cp -r ./final_model_early_stopped /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "0jOhA-UbU0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0T-q_2Dclpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
      ],
      "metadata": {
        "id": "abgP17G0bRlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EHb2g5EObatF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from natasha import MorphVocab, Doc, NewsEmbedding, NewsMorphTagger, Segmenter\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# ======= Natasha =======\n",
        "segmenter = Segmenter()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "# ======= –°—Ç–æ–ø-—Å–ª–æ–≤–∞ =======\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "custom_stopwords = set([\n",
        "    '—Ç–∞–∫–∂–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã–µ', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–≥–æ–¥–∞',\n",
        "    '–±—É–¥–µ—Ç', '–¥–∞–Ω–Ω—ã–π', '–¥–∞–ª–µ–µ', '–Ω—É–∂–Ω–æ', '–º–æ–∂–µ—Ç','–Ω–æ–≤—ã–π', '–º–æ—á—å',\n",
        "    '—Ä–æ—Å—Å–∏—è', '—Ä–æ—Å—Å–∏–π—Å–∫–∏–π', '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π', '—É–∫—Ä–∞–∏–Ω—Å–∫–∏–π'\n",
        "])\n",
        "stop_words = set(russian_stopwords).union(custom_stopwords)\n",
        "\n",
        "# ======= –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ =======\n",
        "def preprocess(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[^–ê-–Ø–∞-—è–Å—ë ]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if not word[0].isupper()]\n",
        "    text = ' '.join(tokens)\n",
        "    text = text.lower()\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    lemmas = []\n",
        "    for token in doc.tokens:\n",
        "        if token.pos not in ['CONJ', 'ADP', 'PRCL', 'INTJ'] and \\\n",
        "           len(token.text) > 3 and token.pos != 'PROPN':\n",
        "            token.lemmatize(morph_vocab)\n",
        "            lemma = token.lemma\n",
        "            if lemma not in stop_words and len(lemma) > 3:\n",
        "                lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "# ======= –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É =======\n",
        "df['lemmas'] = df['text'].progress_apply(preprocess)\n",
        "\n",
        "# ======= –°—á–∏—Ç–∞–µ–º —Å–ª–æ–≤–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º =======\n",
        "theme_word_counts = {}\n",
        "\n",
        "for theme, group in df.groupby('category'):\n",
        "    words = []\n",
        "    for lemmas in group['lemmas']:\n",
        "        words.extend(lemmas)\n",
        "    word_freq = Counter(words)\n",
        "    theme_word_counts[theme] = word_freq.most_common(20)\n",
        "\n",
        "# ======= –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ =======\n",
        "for theme, words in theme_word_counts.items():\n",
        "    print(f\"\\n–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {theme}\")\n",
        "    for word, count in words:\n",
        "        print(f\"  {word}: {count}\")\n"
      ],
      "metadata": {
        "id": "vv0XF9T1bG4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}