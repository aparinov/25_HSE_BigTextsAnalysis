{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"parsed_data.xlsx\")"
      ],
      "metadata": {
        "id": "swAxQZJjNaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "f0q4z-EnXSmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['title', 'overview', 'text'], how='all')"
      ],
      "metadata": {
        "id": "LUkzthu8QM26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "1YFeLTBDSNOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['title', 'overview', 'text']] = df[['title', 'overview', 'text']].fillna('')\n",
        "\n",
        "df['fulltext'] = df['title'] + ' ' + df['overview'] + ' ' + df['text']"
      ],
      "metadata": {
        "id": "jxm9L09aQ4xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "yXS0su-_QQdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MTN1zEUYO30K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3XH0pSXGPLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analitics"
      ],
      "metadata": {
        "id": "CrUyUK6HnpAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤\n",
        "category_counts = df['category'].value_counts(normalize=True)\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
        "duplicate_count = df.duplicated(subset=['fulltext']).sum()\n",
        "\n",
        "# Category distribution visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "category_counts.plot(kind='bar')\n",
        "plt.title('Category Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Category')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –ü–µ—á–∞—Ç–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã\n",
        "print(\"=== –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è BERT ===\")\n",
        "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(df)}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {df['category'].nunique()}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –ø–æ–ª–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É: {duplicate_count}\")\n",
        "print(\"\\n=== –¢–æ–ø-10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É:\")\n",
        "print(df['category'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "AsEBdub9V938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "3B7TRWNenhwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "model_name = 'DeepPavlov/rubert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ä–µ–∑\n",
        "sample_counts = {\n",
        "    '–ü–æ–ª–∏—Ç–∏–∫–∞': 1000,\n",
        "    '–û–±—â–µ—Å—Ç–≤–æ': 500,\n",
        "    '–°–ø–æ—Ä—Ç': 500,\n",
        "    '–ê–≤—Ç–æ': 500,\n",
        "    '–ë–∏–∑–Ω–µ—Å': 100,\n",
        "    '–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –º–µ–¥–∏–∞': 100,\n",
        "    '–≠–∫–æ–Ω–æ–º–∏–∫–∞': 100,\n",
        "    '–§–∏–Ω–∞–Ω—Å—ã': 100,\n",
        "    '–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π': 50\n",
        "}\n",
        "\n",
        "train_df = pd.concat([\n",
        "    df[df['category'] == cat].sample(n=sample_counts[cat], random_state=42)\n",
        "    for cat in sample_counts\n",
        "])\n",
        "\n",
        "test_df = df.drop(train_df.index)\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –º–µ—Ç–æ–∫\n",
        "train_df['fulltext'] = (\n",
        "    train_df['title'].fillna('') + ' ' +\n",
        "    train_df['overview'].fillna('') + ' ' +\n",
        "    train_df['text'].fillna('')\n",
        ")\n",
        "test_df['fulltext'] = (\n",
        "    test_df['title'].fillna('') + ' ' +\n",
        "    test_df['overview'].fillna('') + ' ' +\n",
        "    test_df['text'].fillna('')\n",
        ")\n",
        "\n",
        "label2id = {label: idx for idx, label in enumerate(train_df['category'].unique())}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "train_df['label'] = train_df['category'].map(label2id)\n",
        "test_df['label'] = test_df['category'].map(label2id)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
        "train_dataset = Dataset.from_pandas(train_df[['fulltext', 'label']])\n",
        "eval_dataset = Dataset.from_pandas(test_df[['fulltext', 'label']])\n",
        "\n",
        "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['fulltext'], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "V4kuxG2kNXU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ruBERT model"
      ],
      "metadata": {
        "id": "PO_hiuzQnSrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
        "\n",
        "# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–æ–¥–Ω–∞ —ç–ø–æ—Ö–∞ –∑–∞ —Ä–∞–∑)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# –¶–∏–∫–ª –ø–æ —ç–ø–æ—Ö–∞–º\n",
        "num_epochs = 20\n",
        "acc_history = []\n",
        "f1_history = []\n",
        "\n",
        "best_acc = 0.0\n",
        "best_f1 = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n –≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}\")\n",
        "    trainer.train()\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    acc_history.append(acc)\n",
        "    f1_history.append(f1)\n",
        "\n",
        "    print(f\" Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")\n",
        "\n",
        "    #Classification report\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=[id2label[i] for i in range(len(id2label))]))\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –∏ accuracy –∏ f1 —É–ª—É—á—à–∏–ª–∏—Å—å\n",
        "    if acc > best_acc and f1 > best_f1:\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "        model.save_pretrained('./best_model')\n",
        "        tokenizer.save_pretrained('./best_model')\n",
        "        print(\" –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./best_model\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "model.save_pretrained('./rbc_bert_classifier_new_data')\n",
        "tokenizer.save_pretrained('./rbc_bert_classifier_new_data')\n",
        "print(\" –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./rbc_bert_classifier_new_data\")\n",
        "\n",
        "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), acc_history, label='Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), f1_history, label='Macro F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Accuracy & F1 per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hjbf4emBRkzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./best_model /content/drive/MyDrive/\n",
        "!cp -r ./rbc_bert_classifier_new_data /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "0jOhA-UbU0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0T-q_2Dclpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert Topic. Does not in work just try"
      ],
      "metadata": {
        "id": "EMCETtafhoa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
        "# !pip install -q bertopic[visualization]\n",
        "# !pip install -q sentence-transformers\n",
        "# !pip install -q umap-learn"
      ],
      "metadata": {
        "id": "ZbIQusOkhlbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "data['full_text'] = data[['title', 'overview', 'text']].fillna('').agg(' '.join, axis=1)\n",
        "data['clean_text'] = data['full_text'].apply(clean_text)\n",
        "\n",
        "# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ Sentence-BERT\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "documents = data['clean_text'].tolist()\n",
        "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
        "\n",
        "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BERTopic\n",
        "topic_model = BERTopic(language=\"multilingual\", embedding_model=embedding_model)\n",
        "topics, probs = topic_model.fit_transform(documents, embeddings)\n",
        "\n",
        "# –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ç–µ–º—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ data\n",
        "data['topic'] = topics\n",
        "\n",
        "# –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–º—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\n",
        "topic_to_category = (\n",
        "    data.groupby('topic')['category']\n",
        "    .agg(lambda x: x.value_counts().index[0])\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# –ó–∞–º–µ–Ω—è–µ–º –Ω–æ–º–µ—Ä —Ç–µ–º—ã –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
        "data['predicted_category'] = data['topic'].map(topic_to_category)\n",
        "\n",
        "# –ó–∞–º–µ–Ω–∏–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ–º –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏\n",
        "topic_model.set_topic_labels(topic_to_category)\n",
        "\n",
        "# –°–æ–∫—Ä–∞—â–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–º –¥–æ 10\n",
        "topic_model.reduce_topics(documents, nr_topics=10)\n",
        "\n",
        "# –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ø–∏–∫–∏ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ—Å–ª–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è\n",
        "data['reduced_topic'] = topic_model.topics_\n",
        "reduced_topic_to_category = (\n",
        "    data.groupby('reduced_topic')['category']\n",
        "    .agg(lambda x: x.value_counts().index[0])\n",
        "    .to_dict()\n",
        ")\n",
        "data['predicted_category'] = data['reduced_topic'].map(reduced_topic_to_category)\n",
        "topic_model.set_topic_labels(reduced_topic_to_category)"
      ],
      "metadata": {
        "id": "_4_vUDvabA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()\n"
      ],
      "metadata": {
        "id": "y-6ef40vcz3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=10)\n"
      ],
      "metadata": {
        "id": "rphR6AyedC8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "comparison_df = data.dropna(subset=['category', 'predicted_category'])\n",
        "acc = accuracy_score(comparison_df['category'], comparison_df['predicted_category'])\n",
        "print(f\"\\nüéØ Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(comparison_df['category'], comparison_df['predicted_category'], zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(comparison_df['category'], comparison_df['predicted_category'], labels=comparison_df['category'].unique())\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=comparison_df['category'].unique(), yticklabels=comparison_df['category'].unique(), cmap='Blues')\n",
        "plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è')\n",
        "plt.ylabel('–†–µ–∞–ª—å–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è')\n",
        "plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Confusion Matrix)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6mFRMkHCfgo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"bertopic_model\")\n",
        "data[['title', 'topic', 'full_text']].to_csv(\"texts_with_topics.csv\", index=False)"
      ],
      "metadata": {
        "id": "1TosW2oYdUaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}