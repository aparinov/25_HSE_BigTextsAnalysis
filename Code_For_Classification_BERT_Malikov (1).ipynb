{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"parsed_data.xlsx\")"
      ],
      "metadata": {
        "id": "swAxQZJjNaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "f0q4z-EnXSmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['title', 'overview', 'text'], how='all')"
      ],
      "metadata": {
        "id": "LUkzthu8QM26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "1YFeLTBDSNOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['title', 'overview', 'text']] = df[['title', 'overview', 'text']].fillna('')\n",
        "\n",
        "df['fulltext'] = df['title'] + ' ' + df['overview'] + ' ' + df['text']"
      ],
      "metadata": {
        "id": "jxm9L09aQ4xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "yXS0su-_QQdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MTN1zEUYO30K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "# !pip install --upgrade transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3XH0pSXGPLPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analitics"
      ],
      "metadata": {
        "id": "CrUyUK6HnpAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Инициализируем токенайзер BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Проверка баланса классов\n",
        "category_counts = df['category'].value_counts(normalize=True)\n",
        "\n",
        "# Проверка дубликатов\n",
        "duplicate_count = df.duplicated(subset=['fulltext']).sum()\n",
        "\n",
        "# Category distribution visualization\n",
        "plt.figure(figsize=(10, 5))\n",
        "category_counts.plot(kind='bar')\n",
        "plt.title('Category Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Category')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Печатаем основные выводы\n",
        "print(\"=== Анализ текста для BERT ===\")\n",
        "print(f\"Общее количество текстов: {len(df)}\")\n",
        "print(f\"Количество уникальных категорий: {df['category'].nunique()}\")\n",
        "print(f\"Количество дубликатов по полному тексту: {duplicate_count}\")\n",
        "print(\"\\n=== Топ-10 категорий по количеству:\")\n",
        "print(df['category'].value_counts().head(10))"
      ],
      "metadata": {
        "id": "AsEBdub9V938"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "3B7TRWNenhwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Загрузка модели и токенизатора\n",
        "model_name = 'DeepPavlov/rubert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Сбалансированный срез\n",
        "sample_counts = {\n",
        "    'Политика': 1000,\n",
        "    'Общество': 500,\n",
        "    'Спорт': 500,\n",
        "    'Авто': 500,\n",
        "    'Бизнес': 100,\n",
        "    'Технологии и медиа': 100,\n",
        "    'Экономика': 100,\n",
        "    'Финансы': 100,\n",
        "    'База знаний': 50\n",
        "}\n",
        "\n",
        "train_df = pd.concat([\n",
        "    df[df['category'] == cat].sample(n=sample_counts[cat], random_state=42)\n",
        "    for cat in sample_counts\n",
        "])\n",
        "\n",
        "test_df = df.drop(train_df.index)\n",
        "\n",
        "# Подготовка текста и меток\n",
        "train_df['fulltext'] = (\n",
        "    train_df['title'].fillna('') + ' ' +\n",
        "    train_df['overview'].fillna('') + ' ' +\n",
        "    train_df['text'].fillna('')\n",
        ")\n",
        "test_df['fulltext'] = (\n",
        "    test_df['title'].fillna('') + ' ' +\n",
        "    test_df['overview'].fillna('') + ' ' +\n",
        "    test_df['text'].fillna('')\n",
        ")\n",
        "\n",
        "label2id = {label: idx for idx, label in enumerate(train_df['category'].unique())}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "train_df['label'] = train_df['category'].map(label2id)\n",
        "test_df['label'] = test_df['category'].map(label2id)\n",
        "\n",
        "# Создание датасетов\n",
        "train_dataset = Dataset.from_pandas(train_df[['fulltext', 'label']])\n",
        "eval_dataset = Dataset.from_pandas(test_df[['fulltext', 'label']])\n",
        "\n",
        "# Токенизация\n",
        "def tokenize(example):\n",
        "    return tokenizer(example['fulltext'], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "V4kuxG2kNXU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ruBERT model"
      ],
      "metadata": {
        "id": "PO_hiuzQnSrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Загрузка модели\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))\n",
        "\n",
        "# Аргументы обучения (одна эпоха за раз)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Цикл по эпохам\n",
        "num_epochs = 20\n",
        "acc_history = []\n",
        "f1_history = []\n",
        "\n",
        "best_acc = 0.0\n",
        "best_f1 = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n Эпоха {epoch+1}/{num_epochs}\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Предсказания\n",
        "    predictions = trainer.predict(eval_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    # Метрики\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    acc_history.append(acc)\n",
        "    f1_history.append(f1)\n",
        "\n",
        "    print(f\" Accuracy: {acc:.4f}, Macro F1: {f1:.4f}\")\n",
        "\n",
        "    #Classification report\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=[id2label[i] for i in range(len(id2label))]))\n",
        "\n",
        "    # Сохраняем модель если и accuracy и f1 улучшились\n",
        "    if acc > best_acc and f1 > best_f1:\n",
        "        best_acc = acc\n",
        "        best_f1 = f1\n",
        "        model.save_pretrained('./best_model')\n",
        "        tokenizer.save_pretrained('./best_model')\n",
        "        print(\" Лучшая модель сохранена в ./best_model\")\n",
        "\n",
        "# Сохраняем финальную модель\n",
        "model.save_pretrained('./rbc_bert_classifier_new_data')\n",
        "tokenizer.save_pretrained('./rbc_bert_classifier_new_data')\n",
        "print(\" Финальная модель сохранена в ./rbc_bert_classifier_new_data\")\n",
        "\n",
        "# Построение графиков\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), acc_history, label='Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), f1_history, label='Macro F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Accuracy & F1 per Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hjbf4emBRkzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r ./best_model /content/drive/MyDrive/\n",
        "!cp -r ./rbc_bert_classifier_new_data /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "0jOhA-UbU0Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0T-q_2Dclpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert Topic. Does not in work just try"
      ],
      "metadata": {
        "id": "EMCETtafhoa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 📦 Установка зависимостей\n",
        "# !pip install -q bertopic[visualization]\n",
        "# !pip install -q sentence-transformers\n",
        "# !pip install -q umap-learn"
      ],
      "metadata": {
        "id": "ZbIQusOkhlbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "data['full_text'] = data[['title', 'overview', 'text']].fillna('').agg(' '.join, axis=1)\n",
        "data['clean_text'] = data['full_text'].apply(clean_text)\n",
        "\n",
        "# Эмбеддинги через Sentence-BERT\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "documents = data['clean_text'].tolist()\n",
        "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
        "\n",
        "# Построение BERTopic\n",
        "topic_model = BERTopic(language=\"multilingual\", embedding_model=embedding_model)\n",
        "topics, probs = topic_model.fit_transform(documents, embeddings)\n",
        "\n",
        "# Присваиваем темы обратно к data\n",
        "data['topic'] = topics\n",
        "\n",
        "# Сопоставляем темы с категориями\n",
        "topic_to_category = (\n",
        "    data.groupby('topic')['category']\n",
        "    .agg(lambda x: x.value_counts().index[0])\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Заменяем номер темы на название категории\n",
        "data['predicted_category'] = data['topic'].map(topic_to_category)\n",
        "\n",
        "# Заменим названия тем внутри модели\n",
        "topic_model.set_topic_labels(topic_to_category)\n",
        "\n",
        "# Сокращаем количество тем до 10\n",
        "topic_model.reduce_topics(documents, nr_topics=10)\n",
        "\n",
        "# Перезаписываем топики и категории после сокращения\n",
        "data['reduced_topic'] = topic_model.topics_\n",
        "reduced_topic_to_category = (\n",
        "    data.groupby('reduced_topic')['category']\n",
        "    .agg(lambda x: x.value_counts().index[0])\n",
        "    .to_dict()\n",
        ")\n",
        "data['predicted_category'] = data['reduced_topic'].map(reduced_topic_to_category)\n",
        "topic_model.set_topic_labels(reduced_topic_to_category)"
      ],
      "metadata": {
        "id": "_4_vUDvabA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_topics()\n"
      ],
      "metadata": {
        "id": "y-6ef40vcz3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=10)\n"
      ],
      "metadata": {
        "id": "rphR6AyedC8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "comparison_df = data.dropna(subset=['category', 'predicted_category'])\n",
        "acc = accuracy_score(comparison_df['category'], comparison_df['predicted_category'])\n",
        "print(f\"\\n🎯 Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n📊 Classification Report:\")\n",
        "print(classification_report(comparison_df['category'], comparison_df['predicted_category'], zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(comparison_df['category'], comparison_df['predicted_category'], labels=comparison_df['category'].unique())\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=comparison_df['category'].unique(), yticklabels=comparison_df['category'].unique(), cmap='Blues')\n",
        "plt.xlabel('Предсказанная категория')\n",
        "plt.ylabel('Реальная категория')\n",
        "plt.title('Матрица ошибок (Confusion Matrix)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6mFRMkHCfgo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.save(\"bertopic_model\")\n",
        "data[['title', 'topic', 'full_text']].to_csv(\"texts_with_topics.csv\", index=False)"
      ],
      "metadata": {
        "id": "1TosW2oYdUaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}